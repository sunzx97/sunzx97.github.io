<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>video captioning | sunzx&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="video captioning 是一项跨模态任务，输入视频，输出自然语言来描述视频内容。video captioning的pipeline如图所示。首先使用预训练的模型(如resnet)提取视觉特征，然后通过编码器对视觉特征进一步融合，最后使用解码器生成句子。以下结合论文阐述video captioning上一些重要的工作，image captioning 与 video captioning">
<meta property="og:type" content="article">
<meta property="og:title" content="video captioning">
<meta property="og:url" content="http://example.com/2022/12/10/video-caption/index.html">
<meta property="og:site_name" content="sunzx&#39;s blog">
<meta property="og:description" content="video captioning 是一项跨模态任务，输入视频，输出自然语言来描述视频内容。video captioning的pipeline如图所示。首先使用预训练的模型(如resnet)提取视觉特征，然后通过编码器对视觉特征进一步融合，最后使用解码器生成句子。以下结合论文阐述video captioning上一些重要的工作，image captioning 与 video captioning">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/video_captioning_pipeline.png">
<meta property="og:image" content="http://example.com/images/show_and_tell.png">
<meta property="og:image" content="http://example.com/images/sequence_to_sequence.png">
<meta property="og:image" content="http://example.com/images/show_attend_tell_1.png">
<meta property="og:image" content="http://example.com/images/show_attend_tell_2.png">
<meta property="og:image" content="http://example.com/images/know_when_to_look.png">
<meta property="og:image" content="http://example.com/images/knwo_when_to_look_framework.png">
<meta property="og:image" content="http://example.com/images/AOA.png">
<meta property="og:image" content="http://example.com/images/reconstruction_network.png">
<meta property="og:image" content="http://example.com/images/meshed_transformer.png">
<meta property="og:image" content="http://example.com/images/unified_framework.png">
<meta property="og:image" content="http://example.com/images/unsupervised_framework.png">
<meta property="og:image" content="http://example.com/images/bottom_up_top_down.png">
<meta property="og:image" content="http://example.com/images/aligning_linguistic_words_visual.png">
<meta property="og:image" content="http://example.com/images/aligning_linguistic_word_visual_framework.png">
<meta property="og:image" content="http://example.com/images/better_use_caption.png">
<meta property="og:image" content="http://example.com/images/object_relation_graph.png">
<meta property="og:image" content="http://example.com/images/m3.png">
<meta property="og:image" content="http://example.com/images/pos.png">
<meta property="og:image" content="http://example.com/images/org.png">
<meta property="og:image" content="http://example.com/images/scn.png">
<meta property="og:image" content="http://example.com/images/bridging_by_word.png">
<meta property="og:image" content="http://example.com/images/bridging_gap.png">
<meta property="og:image" content="http://example.com/images/scs.png">
<meta property="og:image" content="http://example.com/images/scs_example.png">
<meta property="og:image" content="http://example.com/images/na.png">
<meta property="og:image" content="http://example.com/images/clip.png">
<meta property="og:image" content="http://example.com/images/prompt_caption.png">
<meta property="article:published_time" content="2022-12-10T09:42:07.000Z">
<meta property="article:modified_time" content="2022-12-28T17:13:43.420Z">
<meta property="article:author" content="sunzx">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/video_captioning_pipeline.png">
  
    <link rel="alternate" href="/atom.xml" title="sunzx's blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">sunzx&#39;s blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-video-caption" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/12/10/video-caption/" class="article-date">
  <time class="dt-published" datetime="2022-12-10T09:42:07.000Z" itemprop="datePublished">2022-12-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      video captioning
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>video captioning 是一项跨模态任务，输入视频，输出自然语言来描述视频内容。video captioning的pipeline如图所示。首先使用预训练的模型(如resnet)提取视觉特征，然后通过编码器对视觉特征进一步融合，最后使用解码器生成句子。<br><img src="/../images/video_captioning_pipeline.png"><br>以下结合论文阐述video captioning上一些重要的工作，image captioning 与 video captioning 有着高度相似，因此以下会参杂着一些image captioning的文章。</p>
<h2 id="Show-and-Tell-A-Neural-Image-Caption-Generator-2015"><a href="#Show-and-Tell-A-Neural-Image-Caption-Generator-2015" class="headerlink" title="Show and Tell: A Neural Image Caption Generator[2015]"></a>Show and Tell: A Neural Image Caption Generator[2015]</h2><p>image captioning方向的早期工作，使用预训练的VGG模型提取图像特征，输入到LSTM中生成句子<br><img src="/../images/show_and_tell.png"></p>
<h2 id="Sequence-to-Sequence-–-Video-to-Text-2015"><a href="#Sequence-to-Sequence-–-Video-to-Text-2015" class="headerlink" title="Sequence to Sequence – Video to Text[2015]"></a>Sequence to Sequence – Video to Text[2015]</h2><p>video captioning方向的早期工作，同样是使用预训练模型提取图像特征后输入到LSTM中生成句子，与 image captioning 不同的是使用了双层LSTM来融合不同视频帧的特征<br><img src="/../images/sequence_to_sequence.png"></p>
<h2 id="Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention-2016"><a href="#Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention-2016" class="headerlink" title="Show, Attend and Tell: Neural Image Caption Generation with Visual Attention [2016]"></a>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention [2016]</h2><p>将注意力机制引入到image captioning，其出发点是生成每个词的时候提取最相关的视觉特征。注意力机制是一个重要的研究点，结合任务产生了各种不同的注意力机制。<br><img src="/../images/show_attend_tell_1.png"><br><img src="/../images/show_attend_tell_2.png"></p>
<h2 id="Knowing-When-to-Look-Adaptive-Attention-via-A-Visual-Sentinel-for-Image-Captioning-2017"><a href="#Knowing-When-to-Look-Adaptive-Attention-via-A-Visual-Sentinel-for-Image-Captioning-2017" class="headerlink" title="Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning [2017]"></a>Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning [2017]</h2><p>注意力机制的改进之一，出发点在于句子中的连接词是与视觉特征无关的，更多取决于句子中的语法结构，其动机图如下<br><img src="/../images/know_when_to_look.png"><br>其实现方式也值得借鉴，直接增加一个表示句子内容的特征矢量<br><img src="/../images/knwo_when_to_look_framework.png"></p>
<h2 id="Attention-on-Attention-for-Image-Captioning-2019"><a href="#Attention-on-Attention-for-Image-Captioning-2019" class="headerlink" title="Attention on Attention for Image Captioning [2019]"></a>Attention on Attention for Image Captioning [2019]</h2><p>同样是对注意力机制的改进，并且效果还行，被后续挺多文章引入到框架中<br><img src="/../images/AOA.png"><br>关于动机，直接引用原文的措辞：to measure the relevance between the attention result and the query<br>另外还有很多其他关于注意力机制的改进，如X-Linear Attention Networks for Image Captioning [2020]，Motion Guided Spatial Attention for Video Captioning [2019]，More Grounded Image Captioning by Distilling Image-Text Matching Model [2020]等，本文不作一一阐述。</p>
<h2 id="Reconstruction-Network-for-Video-Captioning-2018"><a href="#Reconstruction-Network-for-Video-Captioning-2018" class="headerlink" title="Reconstruction Network for Video Captioning [2018]"></a>Reconstruction Network for Video Captioning [2018]</h2><p>在结构上面，该文在编码器解码器后添加了重构器形成了闭环，即通过生成的文字特征重构视觉特征。<br><img src="/../images/reconstruction_network.png"></p>
<h2 id="Meshed-Memory-Transformer-for-Image-Captioning-2020"><a href="#Meshed-Memory-Transformer-for-Image-Captioning-2020" class="headerlink" title="Meshed-Memory Transformer for Image Captioning [2020]"></a>Meshed-Memory Transformer for Image Captioning [2020]</h2><p>随着transformer在自然语言方向的应用，近年transformer也成为了image captioning和video captioning的主流框架。<br><img src="/../images/meshed_transformer.png"></p>
<h2 id="Unified-Vision-Language-Pre-Training-for-Image-Captioning-and-VQA-2019"><a href="#Unified-Vision-Language-Pre-Training-for-Image-Captioning-and-VQA-2019" class="headerlink" title="Unified Vision-Language Pre-Training for Image Captioning and VQA [2019]"></a>Unified Vision-Language Pre-Training for Image Captioning and VQA [2019]</h2><p>统一框架也是研究热点，如将captioning和VQA融合到一个框架中，关于统一框架建议先看 Unified Language Model Pre-training for Natural Language Understanding and Generation [2019]<br><img src="/../images/unified_framework.png"></p>
<h2 id="Towards-Unsupervised-Image-Captioning-with-Shared-Multimodal-Embeddings-2019"><a href="#Towards-Unsupervised-Image-Captioning-with-Shared-Multimodal-Embeddings-2019" class="headerlink" title="Towards Unsupervised Image Captioning with Shared Multimodal Embeddings [2019]"></a>Towards Unsupervised Image Captioning with Shared Multimodal Embeddings [2019]</h2><p>除了使用监督学习的方式，无监督学习框架在captioning也有应用，可以参考 unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks[2017]<br><img src="/../images/unsupervised_framework.png"></p>
<h2 id="Bottom-Up-and-Top-Down-Attention-for-Image-Captioning-and-Visual-Question-Answering-2018"><a href="#Bottom-Up-and-Top-Down-Attention-for-Image-Captioning-and-Visual-Question-Answering-2018" class="headerlink" title="Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering [2018]"></a>Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering [2018]</h2><p>captioning作为一个图像到文本的跨模态任务，更有效的图像提取特征方式能够提高其指标，该文使用了目标检测网络提取图像特征。<br><img src="/../images/bottom_up_top_down.png"></p>
<h2 id="Aligning-Linguistic-Words-and-Visual-Semantic-Units-for-Image-Captioning-2019"><a href="#Aligning-Linguistic-Words-and-Visual-Semantic-Units-for-Image-Captioning-2019" class="headerlink" title="Aligning Linguistic Words and Visual Semantic Units for Image Captioning [2019]"></a>Aligning Linguistic Words and Visual Semantic Units for Image Captioning [2019]</h2><p>该文在视觉特征提取上更进一步，除了提取object feature，还提取了object之间的关系特征，关于这个关系特征如何提取参考：Neural Motifs: Scene Graph Parsing with Global Context [2017]<br><img src="/../images/aligning_linguistic_words_visual.png"><br>最后其网络框架如图<br><img src="/../images/aligning_linguistic_word_visual_framework.png"></p>
<h2 id="Improving-Image-Captioning-with-Better-Use-of-Captions-2020"><a href="#Improving-Image-Captioning-with-Better-Use-of-Captions-2020" class="headerlink" title="Improving Image Captioning with Better Use of Captions [2020]"></a>Improving Image Captioning with Better Use of Captions [2020]</h2><p>在视觉object关系特征提取上，上文用的是Motifs以监督的方式生成的，该文提出从captioning端也构建一个场景图来指导视觉场景图的生成<br><img src="/../images/better_use_caption.png"><br>视觉关系应用到基础的检测网络框架也有相关尝试，参见：Visual Commonsense R-CNN [2020]</p>
<h2 id="Object-Relational-Graph-with-Teacher-Recommended-Learning-for-Video-Captioning-2020"><a href="#Object-Relational-Graph-with-Teacher-Recommended-Learning-for-Video-Captioning-2020" class="headerlink" title="Object Relational Graph with Teacher-Recommended Learning for Video Captioning [2020]"></a>Object Relational Graph with Teacher-Recommended Learning for Video Captioning [2020]</h2><p>在image captioning方向可以使用有监督的方式来生成场景图信息，从而利用其中的关系特征，但是在video captioning就难以生成相应的时间和空间关系特征了，该文直接将其包含在网络结构中让其自动学习object之间的关系特征，非常巧妙<br><img src="/../images/object_relation_graph.png"></p>
<h2 id="M3-Multimodal-Memory-Modelling-for-Video-Captioning-2018"><a href="#M3-Multimodal-Memory-Modelling-for-Video-Captioning-2018" class="headerlink" title="M3: Multimodal Memory Modelling for Video Captioning [2018]"></a>M3: Multimodal Memory Modelling for Video Captioning [2018]</h2><p>captioning任务作为一个文本生成任务，很多machine translation任务存在的问题及解决方法对captioning任务同样有效，如使用记忆网络来解决长时序依赖问题，对应在machine translation的文章为Neural Turing Machines [2014]<br><img src="/../images/m3.png"></p>
<h2 id="Controllable-Video-Captioning-with-POS-Sequence-Guidance-Based-on-Gated-Fusion-Network-2019"><a href="#Controllable-Video-Captioning-with-POS-Sequence-Guidance-Based-on-Gated-Fusion-Network-2019" class="headerlink" title="Controllable Video Captioning with POS Sequence Guidance Based on Gated Fusion Network [2019]"></a>Controllable Video Captioning with POS Sequence Guidance Based on Gated Fusion Network [2019]</h2><p>或者是使用POS信息来从语法上约束生成的句子<br><img src="/../images/pos.png"></p>
<h2 id="Object-Relational-Graph-with-Teacher-Recommended-Learning-for-Video-Captioning-2020-1"><a href="#Object-Relational-Graph-with-Teacher-Recommended-Learning-for-Video-Captioning-2020-1" class="headerlink" title="Object Relational Graph with Teacher-Recommended Learning for Video Captioning [2020]"></a>Object Relational Graph with Teacher-Recommended Learning for Video Captioning [2020]</h2><p>针对语料库的长尾效应，使用预训练模型对其进行修正<br><img src="/../images/org.png"></p>
<h2 id="Semantic-Compositional-Networks-for-Visual-Captioning-2017"><a href="#Semantic-Compositional-Networks-for-Visual-Captioning-2017" class="headerlink" title="Semantic Compositional Networks for Visual Captioning [2017]"></a>Semantic Compositional Networks for Visual Captioning [2017]</h2><p>从图像信息中提取出属性信息，指导句子的生成<br><img src="/../images/scn.png"></p>
<h2 id="Bridging-byWord-Image-Grounded-Vocabulary-Construction-for-Visual-Captioning-2019"><a href="#Bridging-byWord-Image-Grounded-Vocabulary-Construction-for-Visual-Captioning-2019" class="headerlink" title="Bridging byWord: Image-Grounded Vocabulary Construction for Visual Captioning [2019]"></a>Bridging byWord: Image-Grounded Vocabulary Construction for Visual Captioning [2019]</h2><p>这篇文章和从图像提取属性信息辅助句子生成类似，不过该文是通过图像来缩小生成句子的词空间，很巧妙。<br><img src="/../images/bridging_by_word.png"></p>
<h2 id="Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation-2019"><a href="#Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation-2019" class="headerlink" title="Bridging the Gap between Training and Inference for Neural Machine Translation [2019]"></a>Bridging the Gap between Training and Inference for Neural Machine Translation [2019]</h2><p>既然是序列生成任务自然存在exposure bias问题，该文在训练与测试的差异及训练难度上取了一个折中(关于训练阶段为什么不直接使用上个一个时间步生成的单词输入到当前时间步，因为这样很难训练好，误差积累太大，难收敛)<br><img src="/../images/bridging_gap.png"></p>
<h2 id="Self-critical-Sequence-Training-for-Image-Captioning-2017"><a href="#Self-critical-Sequence-Training-for-Image-Captioning-2017" class="headerlink" title="Self-critical Sequence Training for Image Captioning [2017]"></a>Self-critical Sequence Training for Image Captioning [2017]</h2><p>其实解决exposure bias问题更有效的方式是使用强化学习，当然一般会先用交叉熵训练好模型，然后交叉熵+强化学习的方式继续训练模型<br><img src="/../images/scs.png"><br><img src="/../images/scs_example.png"></p>
<h2 id="Non-Autoregressive-Coarse-to-Fine-Video-Captioning-2021"><a href="#Non-Autoregressive-Coarse-to-Fine-Video-Captioning-2021" class="headerlink" title="Non-Autoregressive Coarse-to-Fine Video Captioning [2021]"></a>Non-Autoregressive Coarse-to-Fine Video Captioning [2021]</h2><p>另外也有非自回归这种方式用于生成句子，这种方式自然不存在exposure bias问题，而且生成速度更快，但由于没有直接建模生成单词之间的依赖关系，一般效果不如自回归生成模型，更多需要参考自然语言方向对其的改进。<br><img src="/../images/na.png"></p>
<h2 id="CLIP-Meets-Video-Captioning-Concept-Aware-Representation-Learning-Does-Matter-2022"><a href="#CLIP-Meets-Video-Captioning-Concept-Aware-Representation-Learning-Does-Matter-2022" class="headerlink" title="CLIP Meets Video Captioning: Concept-Aware Representation Learning Does Matter[2022]"></a>CLIP Meets Video Captioning: Concept-Aware Representation Learning Does Matter[2022]</h2><p>前文提到了captioning的整个pipeline的第一步是用预训练模型提取图像特征，一般使用的预训练模型都是基于图像分类的，而本文将其换成了CLIP并取得了不错的效果。<br>CLIP模型来自论文Learning Transferable Visual Models From Natural Language Supervision[2021], 其框架图和主要流程如下<br><img src="/../images/clip.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># image_encoder - ResNet or Vision Transformer</span></span><br><span class="line"><span class="comment"># text_encoder - CBOW or Text Transformer</span></span><br><span class="line"><span class="comment"># I[n, h, w, c] - minibatch of aligned images</span></span><br><span class="line"><span class="comment"># T[n, l] - minibatch of aligned texts</span></span><br><span class="line"><span class="comment"># W_i[d_i, d_e] - learned proj of image to embed</span></span><br><span class="line"><span class="comment"># W_t[d_t, d_e] - learned proj of text to embed</span></span><br><span class="line"><span class="comment"># t - learned temperature parameter</span></span><br><span class="line"><span class="comment"># extract feature representations of each modality</span></span><br><span class="line">I_f = image_encoder(I) <span class="comment">#[n, d_i]</span></span><br><span class="line">T_f = text_encoder(T) <span class="comment">#[n, d_t]</span></span><br><span class="line"><span class="comment"># joint multimodal embedding [n, d_e]</span></span><br><span class="line">I_e = l2_normalize(np.dot(I_f, W_i), axis=<span class="number">1</span>)</span><br><span class="line">T_e = l2_normalize(np.dot(T_f, W_t), axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># scaled pairwise cosine similarities [n, n]</span></span><br><span class="line">logits = np.dot(I_e, T_e.T) * np.exp(t)</span><br><span class="line"><span class="comment"># symmetric loss function</span></span><br><span class="line">labels = np.arange(n)</span><br><span class="line">loss_i = cross_entropy_loss(logits, labels, axis=<span class="number">0</span>)</span><br><span class="line">loss_t = cross_entropy_loss(logits, labels, axis=<span class="number">1</span>)</span><br><span class="line">loss = (loss_i + loss_t)/<span class="number">2</span></span><br></pre></td></tr></table></figure>

<h2 id="Controllable-Image-Captioning-via-Prompting-2022"><a href="#Controllable-Image-Captioning-via-Prompting-2022" class="headerlink" title="Controllable Image Captioning via Prompting[2022]"></a>Controllable Image Captioning via Prompting[2022]</h2><p>该文将Prompt应用到image captioning上, controllable也一直是研究热点，关于prompt知识可以查阅 Pre-train prompt and predict A systematic survey of prompting methods in natural language processing，不过不得不说这些超大规模的预训练模型一般玩不起。<br><img src="/../images/prompt_caption.png"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/12/10/video-caption/" data-id="clky8whza000bhcu5hdp0em1v" data-title="video captioning" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/12/21/video-captioning-baseline/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          video_captioning_baseline
        
      </div>
    </a>
  
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">March 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">February 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">January 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/12/">December 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/08/05/%E5%9F%BA%E4%BA%8Epython%E5%8F%8Ac-%E7%9A%84%E6%B5%B7%E5%BA%B7%E6%91%84%E5%83%8F%E5%A4%B4%E4%BA%8C%E6%AC%A1%E5%BC%80%E5%8F%91/">基于python及c++的海康摄像头二次开发</a>
          </li>
        
          <li>
            <a href="/2023/06/21/%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E5%BE%AA%E7%8E%AF%E9%98%9F%E5%88%97/">基于python的循环队列</a>
          </li>
        
          <li>
            <a href="/2023/04/19/pytorch%E5%8F%8A%E5%85%B6%E4%BB%96%E4%B8%80%E4%BA%9B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0python%E5%BA%93%E4%BB%8B%E7%BB%8D/">pytorch及其他一些深度学习python库介绍</a>
          </li>
        
          <li>
            <a href="/2023/04/07/m3u8%E8%A7%86%E9%A2%91%E4%B8%8B%E8%BD%BD/">m3u8视频下载</a>
          </li>
        
          <li>
            <a href="/2023/03/06/diffusion-model/">diffusion_model</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 sunzx<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>